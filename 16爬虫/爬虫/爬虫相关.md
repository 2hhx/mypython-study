[TOC]



# 1、爬虫是啥？

爬虫的比喻：

​		互联网就像一张蜘蛛网，爬虫相当于蜘蛛，数据相当于猎物

​	爬虫的具体定义：

​		模拟浏览器向后端发送请求，获取数据，解析并且获得我想要的数据，然后存储

​	爬虫的价值：

​		数据的价值



​	发送请求--获得数据--解析数据--存储数据

​	bs4,pyquery,re



# 2、http协议里需要关注的



请求：

​	URL：指明了我要取哪里

​	method：

​		GET：

​			传递数据：？&拼在url后面

​		post：

​			请求体：

​				formdata

​				files

​				json

​	请求头：

​		Cookie：

​		Referer：告诉服务器你从哪里来

​		User-Agent:告诉服务器你的身份



响应：

​	**Status Code**：

​		2xx:成功

​		3xx:重定向

​	响应头：

​		**location**：跳转地址

​		set_cookie:设置cookie

​	响应体：

​		1.html代码

​		2.二进制：图片，视频，音乐

​		3.json格式



# 3、 常用请求库、解析库、数据库的用法

## 3.1 常用请求库	测试网站：http://httpbin.org/get

### request库

​	**安装**：pip install requests

​	**使用**：

​		**请求**：

​			**①get请求：**

```
				响应对象 = requests.get(......)

​				**参数：**

​					url：

​					headers = {}

​					cookies = {}

​					params = {}	

​					proxies = {'http'：‘http://端口：ip’}

​					timeout = 0.5

​					allow_redirects = False
```



​			**②post请求：**

```
				响应对象 = requests.post(......)

​				**参数：**

​					url：

​					headers = {}

​					cookies = {}

​					data = {}

​					json = {}

​					files = {‘file’：open（...，‘rb’）}

​					timeout = 0.5

​					allow_redirects = False
```

​		**自动保存cookie的请求：**

```
			session = requests.session（）

​			r = session.get(......)

​			r = session.post(......)
	  补充:(保存cookie到本地)
	  	import http.cookiejar as cookielib
	  	session.cookie = cookielib.LWPCookieJar()
	  	session.cookie.save(filename='1.txt')
	  	
	  	session.cookies.load(filename='1.txt')
```

​		**响应：**

```
			r.url

​			r.text

​			r.encoding = 'gbk'

​			r.content

​			r.json()

​			r.status_code

​			r.headers

​			r.cookies

​			r.history
```



## 3.2 常用解析语法

### css选择器

​	1、类选择器

​	2、id选择器

​	3、标签选择器

​	4、后代选择器

​	5、子选择器

​	6、属性选择器

​	7、群组选择器

​	8、多条件选择器

### xpath选择器

​	略



## 3.3 牛逼的requests-html

​	**安装：** pip install requests-html

​	**使用：**

​		**请求：**

```
			from requests_html import HTMLSession

​			session = HTMLSession()

​			**参数：**

​				browser.args = [

​					'--no-sand',

​					'--user-agent=XXXXX'

​				]

​			响应对象 = session.request（......）

​			响应对象 = session.get（......）

​			响应对象 = session.post（......）
```

​			**参数和requests模块一毛一样**

​		**响应：**

```
			r.url

​			**属性和requests模块一毛一样
```

**

​		**解析：**

​		**html对象属性：**

```
			r.html.absolute_links

​				   .links

​			           .base_url

​			           .html

​			           .text

​			           .encoding = 'gbk'

​			           .raw_html

​			           .qp
```

​		**html对象方法：**

```
			r.html.find('css选择器')

​				   .find('css选择器'，first = True)

​				   .xpath(‘xpath选择器’)

​				   .xpath('‘xpath选择器'，first = True)

​				   .search(‘模板’)

​			           	（‘xxx{}yyy{}’）[0]

​					   （‘xxx{name}yyy{pwd}’）[‘name’]

​				   .search_all('模板')

​				   .render(.....)

​				   	**参数：**

​					    	scripy：“”“ ( ) => {

​										js代码

​										js代码

​									}

​								  ”“”

​						    scrolldow：n

​						    sleep:n

​						    keep_page:True/False
```



```
()=>{
Object.defineProperties(navigator,{
        webdriver:{
        get: () => undefined
        }
    })
```

​		**与浏览器交互    r.html.page.XXX**

```
				asynic def xxx():

​					await r.html.page.XXX

​				session.loop.run....(xxx())

​			.screenshot({'path':路径})

​			.evaluate('''() =>{js代码}’‘’})

​			.cookies()

​			.type('css选择器'，’内容‘，{’delay‘：100})

​			.click('css选择器')

​			.focus('css选择器')

​			.hover('css选择器')

​			.waitForSelector('css选择器')

​			.waitFor(1000)
```

​		**键盘事件	r.html.page.keyboard.XXX**

```
			.down('Shift')

​			.up('Shift')

​			.press('ArrowLeft')

​			.type('喜欢你啊'，{‘delay’:100})
```

​		**鼠标事件	r.html.page.mouse.XXX**

​			

```
			.click(x,y,{
                'button'：'left',
                'click':1
                'delay':0
			})
			.down({'button'：'left'})
			.up({'button'：'left'})
			.move(x,y,{'steps'：1})
```

​			.

## 常用数据库

###mongoDB4.0:

下载:https://www.mongodb.com/

安装:略

**注意:**使用前修改bin目录下配置文件mongodb.cfg,删除最后一行的'mp'字段

####1. 启动服务与终止服务

net start mongodb

net stop mongodb

#### 2.创建管理员用户

use admin

db.createUser({user:"yxp",pwd:"997997",roles:["root"]})

#### 3.使用账户密码连接mongodb

```
mongo -u adminUserName -p userPassword --authenticationDatabase admin
```

#### 数据库

```
show dbs 查看数据库
use db_name 切换数据库
db.dropDatabase()  删数据库(删前要切换)
db.table1.insert({'a':1})  创建数据库(切换到数据库插入数据)
```

#### 表

```
使用前先切换数据库

show tables 查所有的表
db.table1.insert({'b':2})  增加表(表不存在就创建)
db.table1.drop()    删表
```

#### 数据

```
db.test.insert(user0)    插入一条
db.user.insertMany([user1,user2,user3,user4,user5])   插入多条
db.user.find({'name':'alex'})   查xx==xx
db.user.find({'name':{"$ne":'alex'}})   查xx!=xx
db.user.find({'_id':{'$gt':2}})    查xx>xx
db.user.find({"_id":{"$gte":2,}})  查xx>=xx
db.user.find({'_id':{'$lt':3}})  查xx<xx
db.user.find({"_id":{"$lte":2}})  查xx<=xx
db.user.update({'_id':2},{"$set":{"name":"WXX",}})   改数据
db.user.deleteOne({ 'age': 8 })   删第一个匹配
db.user.deleteMany( {'addr.country': 'China'} )  删全部匹配
db.user.deleteMany({})  删所有
```

#### pymongo

```
conn = pymongo.MongoClient(host=host,port=port, username=username, password=password)
db = client["db_name"] 切换数据库
table = db['表名']
table.insert({})  插入数据
table.remove({})   删除数据
table.update({'_id':2},{"$set":{"name":"WXX",}})   改数据
table.find({})  查数据
```



# 爬虫与反爬虫的对抗历史

![1565136366701](1565136366701.png)



# 常见反扒手段

1.检测浏览器headers

2.ip封禁

3.图片验证码

4.滑动模块

5.js轨迹

6.前端反调试



# 小爬爬

1.爬校花图片

2.爬豆瓣电影

3.爬取校花视频

4.爬取天猫

​	反爬虫:使用技术手段防止爬虫程序的方法

​	误伤:反扒技术将普通用户识别为爬虫,如果误伤过高,效果再好也不能用

​	成本:反爬虫需要的人力和机器成本

​	拦截:成功拦截爬虫,一般情况下,拦截率越高,误伤率越高

5.分析腾讯视频url

​	接口：https://p2p.1616jx.com/api/api.php?url=https://v.qq.com/x/cover/m441e3rjq9kwpsc/t0032zc18fo.html

视频连接

​	然后么m3u8

http://www.m3u8.net.cn/

```
https://cn7.7639616.com/hls/20191130/806b481aaeda9e9f5118a455f16d4f36/1575079663/index.m3u8
```

6.登录知乎

​	保存cookie到本地

7.登录拉钩

​	验证码处理:

​		1.自己处理

​		2.在线打码

​		3.人工打码



# scrapy

