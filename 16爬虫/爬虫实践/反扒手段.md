# ![](https://img2018.cnblogs.com/blog/1739658/202001/1739658-20200103185123433-1791345775.png)常见反扒手段

1.检测浏览器headers(user-agent)

2.ip封禁（代理ip）https://www.becmd.com/

http://www.z-sms.com/

3.图片验证码（云打码，打码兔）

4.滑动模块(seleuim进行模拟人的行为)

5.js加密算法（找到js轨迹，加密算法，在python 有模块可以直接执行js       俩个库js2pypyExecjs（推荐）pip install pyExecjs）

5.js轨迹

6.前端反调试（阻止看源码）

# 反爬虫

反爬虫:使用技术手段防止爬虫程序的方法

​	误伤:反扒技术将普通用户识别为爬虫,如果误伤过高,效果再好也不能用

​	成本:反爬虫需要的人力和机器成本

​	拦截:成功拦截爬虫,一般情况下,拦截率越高,误伤率越高

# 2、http协议里需要关注的

## 2.1 请求需要关注的东西 requests

url : 告诉浏览器,你要去哪里

Method:

​	get:

​		数据:url?key=value&key=value

​	post:

​		请求体:

​			form data

​			文件类型

​			json

headers:

​	cookie:保存用户登录状态

​	User-Agent:告诉服务器你是谁

​	refere:告诉服务器你从哪里来

​	服务器规定的特殊字段



## 2.2 请求需要关注的东西 response

Status Code:

​	2xx

​		请求成功(不一定)---后台程序员自己规定的---不能用作请求成功的唯一判断标准

​	3xx

​		重定向

响应头:

​	location:重定向地址

​	set_cookie:设置cookie

​	服务器规定的特殊字段



响应体:

​	1.html代码(css,html,js)

​	2.json

​	3.二进制(图片,视频,音频)

*headers进行反爬是最常见的反爬虫策略。

*还有一些网站会对 Referer （上级链接）进行检测（机器行为不太可能通过链接跳转实现）

从而实现爬虫。



# headers 知识补充

host：提供了主机名及端口号

Referer 提供给服务器客户端从那个页面链接过来的信息（有些网站会据此来反爬）

Origin：Origin字段里只包含是谁发起的请求，并没有其他信息.(仅存于post请求)

User agent: 发送请求的应用程序名（一些网站会根据UA访问的频率间隔时间进行反爬）

proxies： 代理，一些网站会根据ip访问的频率次数等选择封ip.

cookie： 特定的标记信息，一般可以直接复制，对于一些变化的可以选择构造. 

(session=requests.session()自动把cookie信息存入response对象中)

Accept首部为客户端提供了一种将其喜好和能力告知服务器的方式

首部 　　　　　　　　　　 描述

Accept 　　　　　　　　 告诉服务器能够发送哪些媒体类型

Accept-Charset 　　　　 告诉服务器能够发送哪些字符集

Accept-Encoding 　　　　告诉服务器能够发送哪些编码方式(最常见的是utf-8)

Accept-Language 　　　 告诉服务器能够发送哪些语言

Cache-control: 这个字段用于指定所有缓存机制在整个请求/响应链中必须服从的指令



# 三 ip限制

限制ip访问频率和次数进行反爬.

解决措施：构造自己的 IP 代理池，然后每次访问时随机选择代理（但一些 IP 地址不是非常稳定，需要经常检查更新）



# 四 UA限制

UA使用户访问网站时候的浏览器标识.

温馨提醒:

当然如果反爬对时间还有限制的话，可以在requests 设置timeout（最好是随机休眠，这样会更安全稳定，time.sleep()）

解决措施,构建自己的UA池,每次python做requests访问时随机挂上UA标识,更好的模拟浏览器行为.

超级简单的请求头fake_useragent库(https://www.jianshu.com/p/b76df35aec93)

\#随机请求头

import requests

from fake_useragent import UserAgent

ua = UserAgent()

headers = {'User-Agent': ua.random}

url = '待爬网页的url'

resp = requests.get(url, headers=headers)

\##待补充 请求加睡眠时间



# 五.验证码反爬虫或者模拟登陆

图片验证码:通过简单的图像识别是可以完成

验证码识别的基本方法：截图，二值化、中值滤波去噪、分割、紧缩重排（让高矮统一）、字库特征匹配识别。



# 六  Ajax动态加载

Ajax动态加载的工作原理是：从网页的 url 加载网页的源代码之后，会在浏览器里执行JavaScript程序。

这些程序会加载出更多的内容，并把这些内容传输到网页中。这就是为什么有些网页直接爬它的URL时却

没有数据的原因。

解决方案:若使用审查元素分析”请求“对应的链接(方法：右键→审查元素→Network→清空，点击”加载更多

“，出现对应的GET链接寻找Type为text/html的，点击，查看get参数或者复制Request URL)，循环过程。如果“请求”之前有页面，依据上一步的网址进行分析推导第1页。以此类推，抓取抓Ajax地址的数据。对返回的json使用requests中的json进行解析，使用eval（）转成字典处理

抓包工具推荐:fiddler



# 七.cookie限制

一次打开网页会生成一个随机cookie，如果再次打开网页这个cookie不存在，那么再次设置，第三次打开仍然不存在，这就非常有可能是爬虫在工作了.

解决措施：在headers挂上相应的cookie或者根据其方法进行构造

