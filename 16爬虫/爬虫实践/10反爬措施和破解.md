# 爬虫与反爬虫的对抗历史

![](https://img2018.cnblogs.com/blog/1739658/202001/1739658-20200103185123433-1791345775.png)





# 常见反扒手段

1.检测浏览器headers

2.ip封禁

3.图片验证码

4.滑动模块

5.js加密算法

5.js轨迹

6.前端反调试



# 爬取百度图片

```
请求头加入这个是破解百度图片的反爬
'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'
```



```
from urllib.parse import urlencode
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'
}

# https://www.baidu.com/s?wd=美女
response=requests.get('https://www.baidu.com/s?'+urlencode({'wd':'美女'}),headers=headers)
response=requests.get('https://www.baidu.com/s',params={'wd':'美女'},headers=headers) #params内部就是调用urlencode
reponse.encoding = 'utf8'
print(response.text)
```

*headers进行反爬是最常见的反爬虫策略。

*还有一些网站会对 Referer （上级链接）进行检测（机器行为不太可能通过链接跳转实现）

从而实现爬虫。



headers 知识补充***

host：提供了主机名及端口号

Referer 提供给服务器客户端从那个页面链接过来的信息（有些网站会据此来反爬）

Origin：Origin字段里只包含是谁发起的请求，并没有其他信息.(仅存于post请求)

User agent: 发送请求的应用程序名（一些网站会根据UA访问的频率间隔时间进行反爬）

proxies： 代理，一些网站会根据ip访问的频率次数等选择封ip.

cookie： 特定的标记信息，一般可以直接复制，对于一些变化的可以选择构造. 

(session=requests.session()自动把cookie信息存入response对象中)

Accept首部为客户端提供了一种将其喜好和能力告知服务器的方式

首部 　　　　　　　　　　 描述

Accept 　　　　　　　　 告诉服务器能够发送哪些媒体类型

Accept-Charset 　　　　 告诉服务器能够发送哪些字符集

Accept-Encoding 　　　　告诉服务器能够发送哪些编码方式(最常见的是utf-8)

Accept-Language 　　　 告诉服务器能够发送哪些语言

Cache-control: 这个字段用于指定所有缓存机制在整个请求/响应链中必须服从的指令



三 ip限制

限制ip访问频率和次数进行反爬.

解决措施：构造自己的 IP 代理池，然后每次访问时随机选择代理（但一些 IP 地址不是非常稳定，需要经常检查更新）



四 UA限制

UA使用户访问网站时候的浏览器标识.

温馨提醒:

当然如果反爬对时间还有限制的话，可以在requests 设置timeout（最好是随机休眠，这样会更安全稳定，time.sleep()）

解决措施,构建自己的UA池,每次python做requests访问时随机挂上UA标识,更好的模拟浏览器行为.

超级简单的请求头fake_useragent库(https://www.jianshu.com/p/b76df35aec93)

\#随机请求头

import requests

from fake_useragent import UserAgent

ua = UserAgent()

headers = {'User-Agent': ua.random}

url = '待爬网页的url'

resp = requests.get(url, headers=headers)

\##待补充 请求加睡眠时间



五.验证码反爬虫或者模拟登陆

图片验证码:通过简单的图像识别是可以完成

验证码识别的基本方法：截图，二值化、中值滤波去噪、分割、紧缩重排（让高矮统一）、字库特征匹配识别。



六  Ajax动态加载

Ajax动态加载的工作原理是：从网页的 url 加载网页的源代码之后，会在浏览器里执行JavaScript程序。

这些程序会加载出更多的内容，并把这些内容传输到网页中。这就是为什么有些网页直接爬它的URL时却

没有数据的原因。

解决方案:若使用审查元素分析”请求“对应的链接(方法：右键→审查元素→Network→清空，点击”加载更多

“，出现对应的GET链接寻找Type为text/html的，点击，查看get参数或者复制Request URL)，循环过程。如果“请求”之前有页面，依据上一步的网址进行分析推导第1页。以此类推，抓取抓Ajax地址的数据。对返回的json使用requests中的json进行解析，使用eval（）转成字典处理

抓包工具推荐:fiddler



七.cookie限制

一次打开网页会生成一个随机cookie，如果再次打开网页这个cookie不存在，那么再次设置，第三次打开仍然不存在，这就非常有可能是爬虫在工作了.

解决措施：在headers挂上相应的cookie或者根据其方法进行构造