# 1.你觉得爬虫的核心技术体现在哪里？

数据解析和破解反扒。

# 2.你是如何处理网络请求超时的？

1.设置超时处理，超过时间返回异常。

2.将异常的url存储，一段时间后，进行从新尝试。

3.检查是否是由于ip封禁以及cookies过期。



思路一
1.设置超时处理，超过时间返回异常。
2.重试与超时结合。
3.在超时范围内发现问题，及时处理。

思路二
1.将要下载的url形成列表文件；
2.将已下载url记录形成列表文件；
2.出现错误后比较前后两个文件内容，删除重复内容；
4.按照删除重复后的列表文件的继续运行下载程序。



# 3.你是如何处理网页抽取时出现的异常的？

进行分析：看异常并进行分析。

网络无连接，即本机无法上网
连接不到特定的服务器
服务器不存在等



有时候python爬取的网页会出现异常，我们需要添加异常处理

我们主要说明一下URLError和HTTPError

参考博客：点击打开链接

URLError

首先解释下URLError可能产生的原因：

网络无连接，即本机无法上网
连接不到特定的服务器
服务器不存在
HTTPError是URLError的子类，在你利用urlopen方法发出一个请求时，服务器上都会对应一个应答对象response，其中它包含一个数字”状态码”，具体每个状态码代表什么可以去网上查到。

```
from urllib import request
from urllib import error

if __name__ == '__main__':
    target_url = 'http://www.dubai.com/'
    try :
        res = request.urlopen(target_url)
    except error.URLError as e:
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)
        else:
            print("OK")
```



因为HTTPerror是URLerror的子类，所以也可以被URLError获取到，我们也可以分开获取，再写一个except error.HTTPError as e:

```
1.你觉得爬虫的核心技术体现在哪里？
数据解析和破解反扒。
2.你是如何处理网络请求超时的？
设置超时处理，超过时间返回异常。
将异常的url存储，一段时间后，进行从新尝试。
检查是否是由于ip封禁以及cookies过期。
3.你是如何处理网页抽取时出现的异常的？
进行分析：看异常并进行分析。
网络无连接，即本机无法上网
连接不到特定的服务器
服务器不存在等
```

