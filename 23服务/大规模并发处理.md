# Web系统大规模并发处理

**一、大规模并发带来的挑战** 

面对5w每秒的高并发秒杀功能，如果Web系统不做针对性的优化，会轻而易举地陷入到异常状态。我们现在一起来讨论下，优化的思路和方法哈。 

**1. 请求接口的合理设计**

一个秒杀或者抢购页面，通常分为2个部分，一个是静态的HTML等内容，另一个就是参与秒杀的Web后台请求接口。

通常静态HTML等内容，是通过CDN的部署，一般压力不大，核心瓶颈实际上在后台请求接口上。这个后端接口，必须能够支持高并发请求，同时，非常重要的一点，必须尽可能“快”，在最短的时间里返回用户的请求结果。为了实现尽可能快这一点，接口的后端存储使用内存级别的操作会更好一点。仍然直接面向MySQL之类的存储是不合适的，如果有这种复杂业务的需求，都建议采用异步写入。

![img](http://cms.csdnimg.cn/article/201411/28/547821ba53d6d_middle.jpg) 

当然，也有一些秒杀和抢购采用“滞后反馈”，就是说秒杀当下不知道结果，一段时间后才可以从页面中看到用户是否秒杀成功。但是，这种属于“偷懒”行为，同时给用户的体验也不好，容易被用户认为是“暗箱操作”。

**2. 高并发的挑战：一定要“快”**

我们通常衡量一个Web系统的吞吐率的指标是QPS（Query Per Second，每秒处理请求数），解决每秒数万次的高并发场景，这个指标非常关键。举个例子，我们假设处理一个业务请求平均响应时间为100ms，同时，系统内有20台Apache的Web服务器，配置MaxClients为500个（表示Apache的最大连接数目）。

那么，我们的Web系统的理论峰值QPS为（理想化的计算方式）：

20*500/0.1 = 100000 （10万QPS）

咦？我们的系统似乎很强大，1秒钟可以处理完10万的请求，5w/s的秒杀似乎是“纸老虎”哈。实际情况，当然没有这么理想。在高并发的实际场景下，机器都处于高负载的状态，在这个时候平均响应时间会被大大增加。

就Web服务器而言，Apache打开了越多的连接进程，CPU需要处理的上下文切换也越多，额外增加了CPU的消耗，然后就直接导致平均响应时间增加。因此上述的MaxClient数目，要根据CPU、内存等硬件因素综合考虑，绝对不是越多越好。可以通过Apache自带的abench来测试一下，取一个合适的值。然后，我们选择内存操作级别的存储的Redis，在高并发的状态下，存储的响应时间至关重要。网络带宽虽然也是一个因素，不过，这种请求数据包一般比较小，一般很少成为请求的瓶颈。负载均衡成为系统瓶颈的情况比较少，在这里不做讨论哈。

那么问题来了，假设我们的系统，在5w/s的高并发状态下，平均响应时间从100ms变为250ms（实际情况，甚至更多）：

20*500/0.25 = 40000 （4万QPS）

于是，我们的系统剩下了4w的QPS，面对5w每秒的请求，中间相差了1w。

然后，这才是真正的恶梦开始。举个例子，高速路口，1秒钟来5部车，每秒通过5部车，高速路口运作正常。突然，这个路口1秒钟只能通过4部车，车流量仍然依旧，结果必定出现大塞车。（5条车道忽然变成4条车道的感觉）

同理，某一个秒内，20*500个可用连接进程都在满负荷工作中，却仍然有1万个新来请求，没有连接进程可用，系统陷入到异常状态也是预期之内。

![img](http://cms.csdnimg.cn/article/201411/28/547821ee2d1ee_middle.jpg) 

其实在正常的非高并发的业务场景中，也有类似的情况出现，某个业务请求接口出现问题，响应时间极慢，将整个Web请求响应时间拉得很长，逐渐将Web服务器的可用连接数占满，其他正常的业务请求，无连接进程可用。

更可怕的问题是，是用户的行为特点，系统越是不可用，用户的点击越频繁，恶性循环最终导致“雪崩”（其中一台Web机器挂了，导致流量分散到其他正常工作的机器上，再导致正常的机器也挂，然后恶性循环），将整个Web系统拖垮。

**3. 重启与过载保护**

如果系统发生“雪崩”，贸然重启服务，是无法解决问题的。最常见的现象是，启动起来后，立刻挂掉。这个时候，最好在入口层将流量拒绝，然后再将重启。如果是redis/memcache这种服务也挂了，重启的时候需要注意“预热”，并且很可能需要比较长的时间。

秒杀和抢购的场景，流量往往是超乎我们系统的准备和想象的。这个时候，过载保护是必要的。如果检测到系统满负载状态，拒绝请求也是一种保护措施。在前端设置过滤是最简单的方式，但是，这种做法是被用户“千夫所指”的行为。更合适一点的是，将过载保护设置在CGI入口层，快速将客户的直接请求返回。

**二、作弊的手段：进攻与防守**

秒杀和抢购收到了“海量”的请求，实际上里面的水分是很大的。不少用户，为了“抢“到商品，会使用“刷票工具”等类型的辅助工具，帮助他们发送尽可能多的请求到服务器。还有一部分高级用户，制作强大的自动请求脚本。这种做法的理由也很简单，就是在参与秒杀和抢购的请求中，自己的请求数目占比越多，成功的概率越高。

这些都是属于“作弊的手段”，不过，有“进攻”就有“防守”，这是一场没有硝烟的战斗哈。

**1. 同一个账号，一次性发出多个请求**

部分用户通过浏览器的插件或者其他工具，在秒杀开始的时间里，以自己的账号，一次发送上百甚至更多的请求。实际上，这样的用户破坏了秒杀和抢购的公平性。

这种请求在某些没有做数据安全处理的系统里，也可能造成另外一种破坏，导致某些判断条件被绕过。例如一个简单的领取逻辑，先判断用户是否有参与记录，如果没有则领取成功，最后写入到参与记录中。这是个非常简单的逻辑，但是，在高并发的场景下，存在深深的漏洞。多个并发请求通过负载均衡服务器，分配到内网的多台Web服务器，它们首先向存储发送查询请求，然后，在某个请求成功写入参与记录的时间差内，其他的请求获查询到的结果都是“没有参与记录”。这里，就存在逻辑判断被绕过的风险。

![img](http://cms.csdnimg.cn/article/201411/28/5478221b71722_middle.jpg) 



**应对方案：**

在程序入口处，一个账号只允许接受1个请求，其他请求过滤。不仅解决了同一个账号，发送N个请求的问题，还保证了后续的逻辑流程的安全。实现方案，可以通过Redis这种内存缓存服务，写入一个标志位（只允许1个请求写成功，结合watch的乐观锁的特性），成功写入的则可以继续参加。

![img](http://cms.csdnimg.cn/article/201411/28/547822508a00b_middle.jpg) 

或者，自己实现一个服务，将同一个账号的请求放入一个队列中，处理完一个，再处理下一个。

**2. 多个账号，一次性发送多个请求**

很多公司的账号注册功能，在发展早期几乎是没有限制的，很容易就可以注册很多个账号。因此，也导致了出现了一些特殊的工作室，通过编写自动注册脚本，积累了一大批“僵尸账号”，数量庞大，几万甚至几十万的账号不等，专门做各种刷的行为（这就是微博中的“僵尸粉“的来源）。举个例子，例如微博中有转发抽奖的活动，如果我们使用几万个“僵尸号”去混进去转发，这样就可以大大提升我们中奖的概率。

这种账号，使用在秒杀和抢购里，也是同一个道理。例如，iPhone官网的抢购，火车票黄牛党。

![img](http://cms.csdnimg.cn/article/201411/28/547822740404f_middle.jpg) 

**应对方案：**

这种场景，可以通过检测指定机器IP请求频率就可以解决，如果发现某个IP请求频率很高，可以给它弹出一个验证码或者直接禁止它的请求：



1. 弹出验证码，最核心的追求，就是分辨出真实用户。因此，大家可能经常发现，网站弹出的验证码，有些是“鬼神乱舞”的样子，有时让我们根本无法看清。他们这样做的原因，其实也是为了让验证码的图片不被轻易识别，因为强大的“自动脚本”可以通过图片识别里面的字符，然后让脚本自动填写验证码。实际上，有一些非常创新的验证码，效果会比较好，例如给你一个简单问题让你回答，或者让你完成某些简单操作（例如百度贴吧的验证码）。
2. 直接禁止IP，实际上是有些粗暴的，因为有些真实用户的网络场景恰好是同一出口IP的，可能会有“误伤“。但是这一个做法简单高效，根据实际场景使用可以获得很好的效果。



**3. 多个账号，不同IP发送不同请求**

所谓道高一尺，魔高一丈。有进攻，就会有防守，永不休止。这些“工作室”，发现你对单机IP请求频率有控制之后，他们也针对这种场景，想出了他们的“新进攻方案”，就是不断改变IP。

![img](http://cms.csdnimg.cn/article/201411/28/5478229f6aa52_middle.jpg) 

有同学会好奇，这些随机IP服务怎么来的。有一些是某些机构自己占据一批独立IP，然后做成一个随机代理IP的服务，有偿提供给这些“工作室”使用。还有一些更为黑暗一点的，就是通过木马黑掉普通用户的电脑，这个木马也不破坏用户电脑的正常运作，只做一件事情，就是转发IP包，普通用户的电脑被变成了IP代理出口。通过这种做法，黑客就拿到了大量的独立IP，然后搭建为随机IP服务，就是为了挣钱。

**应对方案：**

说实话，这种场景下的请求，和真实用户的行为，已经基本相同了，想做分辨很困难。再做进一步的限制很容易“误伤“真实用户，这个时候，通常只能通过设置业务门槛高来限制这种请求了，或者通过账号行为的”数据挖掘“来提前清理掉它们。

僵尸账号也还是有一些共同特征的，例如账号很可能属于同一个号码段甚至是连号的，活跃度不高，等级低，资料不全等等。根据这些特点，适当设置参与门槛，例如限制参与秒杀的账号等级。通过这些业务手段，也是可以过滤掉一些僵尸号。

**4. 火车票的抢购**

看到这里，同学们是否明白你为什么抢不到火车票？如果你只是老老实实地去抢票，真的很难。通过多账号的方式，火车票的黄牛将很多车票的名额占据，部分强大的黄牛，在处理验证码方面，更是“技高一筹“。

高级的黄牛刷票时，在识别验证码的时候使用真实的人，中间搭建一个展示验证码图片的中转软件服务，真人浏览图片并填写下真实验证码，返回给中转软件。对于这种方式，验证码的保护限制作用被废除了，目前也没有很好的解决方案。

![img](http://cms.csdnimg.cn/article/201411/28/547822ce2f038.jpg) 

因为火车票是根据身份证实名制的，这里还有一个火车票的转让操作方式。大致的操作方式，是先用买家的身份证开启一个抢票工具，持续发送请求，黄牛账号选择退票，然后黄牛买家成功通过自己的身份证购票成功。当一列车厢没有票了的时候，是没有很多人盯着看的，况且黄牛们的抢票工具也很强大，即使让我们看见有退票，我们也不一定能抢得过他们哈。 

![img](http://cms.csdnimg.cn/article/201411/28/547822fbe13a0.jpg) 

最终，黄牛顺利将火车票转移到买家的身份证下。

**解决方案：**

并没有很好的解决方案，唯一可以动心思的也许是对账号数据进行“数据挖掘”，这些黄牛账号也是有一些共同特征的，例如经常抢票和退票，节假日异常活跃等等。将它们分析出来，再做进一步处理和甄别。

**三、高并发下的数据安全**

我们知道在多线程写入同一个文件的时候，会存现“线程安全”的问题（多个线程同时运行同一段代码，如果每次运行结果和单线程运行的结果是一样的，结果和预期相同，就是线程安全的）。如果是MySQL数据库，可以使用它自带的锁机制很好的解决问题，但是，在大规模并发的场景中，是不推荐使用MySQL的。秒杀和抢购的场景中，还有另外一个问题，就是“超发”，如果在这方面控制不慎，会产生发送过多的情况。我们也曾经听说过，某些电商搞抢购活动，买家成功拍下后，商家却不承认订单有效，拒绝发货。这里的问题，也许并不一定是商家奸诈，而是系统技术层面存在超发风险导致的。

**1. 超发的原因**

假设某个抢购场景中，我们一共只有100个商品，在最后一刻，我们已经消耗了99个商品，仅剩最后一个。这个时候，系统发来多个并发请求，这批请求读取到的商品余量都是99个，然后都通过了这一个余量判断，最终导致超发。（同文章前面说的场景）

![img](http://cms.csdnimg.cn/article/201411/28/54782328bb155_middle.jpg) 

在上面的这个图中，就导致了并发用户B也“抢购成功”，多让一个人获得了商品。这种场景，在高并发的情况下非常容易出现。

**2. 悲观锁思路**

解决线程安全的思路很多，可以从“悲观锁”的方向开始讨论。

悲观锁，也就是在修改数据的时候，采用锁定状态，排斥外部请求的修改。遇到加锁的状态，就必须等待。

![img](http://cms.csdnimg.cn/article/201411/28/5478234d530bf_middle.jpg) 

虽然上述的方案的确解决了线程安全的问题，但是，别忘记，我们的场景是“高并发”。也就是说，会很多这样的修改请求，每个请求都需要等待“锁”，某些线程可能永远都没有机会抢到这个“锁”，这种请求就会死在那里。同时，这种请求会很多，瞬间增大系统的平均响应时间，结果是可用连接数被耗尽，系统陷入异常。

**3. FIFO队列思路**

那好，那么我们稍微修改一下上面的场景，我们直接将请求放入队列中的，采用FIFO（First Input First Output，先进先出），这样的话，我们就不会导致某些请求永远获取不到锁。看到这里，是不是有点强行将多线程变成单线程的感觉哈。

![img](http://cms.csdnimg.cn/article/201411/28/547823786991a_middle.jpg?_=10168) 

然后，我们现在解决了锁的问题，全部请求采用“先进先出”的队列方式来处理。那么新的问题来了，高并发的场景下，因为请求很多，很可能一瞬间将队列内存“撑爆”，然后系统又陷入到了异常状态。或者设计一个极大的内存队列，也是一种方案，但是，系统处理完一个队列内请求的速度根本无法和疯狂涌入队列中的数目相比。也就是说，队列内的请求会越积累越多，最终Web系统平均响应时候还是会大幅下降，系统还是陷入异常。

**4. 乐观锁思路**

这个时候，我们就可以讨论一下“乐观锁”的思路了。乐观锁，是相对于“悲观锁”采用更为宽松的加锁机制，大都是采用带版本号（Version）更新。实现就是，这个数据所有请求都有资格去修改，但会获得一个该数据的版本号，只有版本号符合的才能更新成功，其他的返回抢购失败。这样的话，我们就不需要考虑队列的问题，不过，它会增大CPU的计算开销。但是，综合来说，这是一个比较好的解决方案。

![img](http://cms.csdnimg.cn/article/201411/28/547823d8e0db8_middle.jpg) 

有很多软件和服务都“乐观锁”功能的支持，例如Redis中的watch就是其中之一。通过这个实现，我们保证了数据的安全。

**四、小结**

互联网正在高速发展，使用互联网服务的用户越多，高并发的场景也变得越来越多。电商秒杀和抢购，是两个比较典型的互联网高并发场景。虽然我们解决问题的具体技术方案可能千差万别，但是遇到的挑战却是相似的，因此解决问题的思路也异曲同工。





# 网站大流量高并发访问的处理解决办法

1、硬件升级
2、服务器集群、负载均衡、分布式
3、CDN
4、页面静态化
5、缓存技术(Memcache、Redis)
以上为架构层面
以下为网站本地项目层面
6、数据库优化
  1、数据库分表技术
  2、数据库读写分离
  3、表建立相应的索引
7、禁止盗链
8、控制大文件的上传下载

服务器并发处理
1、什么是服务器并发处理能力
  一台服务器在单位时间里能处理的请求越多,服务器的能力越高,也就是服务器并发处理能力越强.HTTP请求通常是对不同资源的请求,也就是请求不同的URL,有的是请求图片,有的是获取动态内容,有的是静态页面,显然这些请求所花费的时间各不相同,而这些请求再不同时间的组成比例又是不确定的.

  说个题外话

  假如100个用户同时向服务器分别进行10个请求，与1个用户向服务器连续进行1000次请求，对服务器的压力是一样吗？(服务器缓冲区只有1个和100个请求等待处理)

2、提高服务器的并发处理能力
提高CUP并发的处理能力

  1.多进程:多进程的好处不仅在于CPU时间的轮流使用,还在于对CPU计算和I/O操作进行很好的重叠利用.

  2.减少进程切换:进程拥有独立的内存空间,每个进程都只能共享CPU寄存器。一个进程被挂起的本质是将它在CPU寄存器中的数据拿出来暂存在内存态堆栈着那个，而一个进程恢复工作的本质就是把它的数据重新装入CPU寄存器,这段装入和移出的数据称为“硬件上下文”,当硬件上下文频繁装入和移出时,所消耗的时间是非常明显的

  3.减少使用不必要的锁:服务器处理大量并发请求时,多个请求处理任务时存在一些资源抢占竞争,这时一般采用“锁”机制来控制资源的占用,当一个任务占用资源时,我们锁住资源,这时其它任务都在等待锁的释放.

  4.其他:不写了.对于老板来说,买就是了o(*￣︶￣*)o

服务器集群、负载均衡、分布式
1、什么是集群？
  单机处理到达瓶颈的时候,你就把单机复制几份,这样就构成了一个“集群”.集群中的每一台服务器叫做这个集群的节点,节点构成集群(废话…).每个节点提供<相同>的业务或者服务.这样系统的处理能力就会翻倍.

  那么问题来了,用户的请求到底哪一台服务器去处理执行的？必须有”领导(负载均衡器)”.这个领导的职责就是进行调度所有的请求以达到使得每一台服务器的负载均衡(是不是很熟悉=.=),不能让有的人闲着,有的人忙死.

2、集群结构的优点？
  集群结构的好处就是系统扩展非常容易.如果随着你们系统业务的发展,当前的系统又支撑不住了,那么给这个集群再增加节点就行了.所有节点处于活动状态,有一台down(宕)机，那么整个的业务还在跑(分布式的话,emmmm….) 

3、集群分类:
  Linux集群主要分成三大类:(高可用集群,负载均衡集群,科学计算集群),其他两个没了解,估计原理差不多吧…

4、集群负载的原理？
DNS轮询、HTTP重定向、IP欺骗（又称三角传输）    (这三种实现方式都是在用户通过域名来访问目标服务器时，由GSLB设备(Global Server Load Balancing)进行智能决策，将用户引导到一个最佳的服务IP)

智能DNS可以通过多种负载均衡策略来将客户端需要访问的域名解析到不同的数据中心不同的线路上，比如通过IP地理信息数据库解析到最近的线路，或者权衡不同线路的繁忙度解析到空闲的线路等等.

下面介绍一个解析线路的过程:

1、DNS的负载均衡:

用户访问某个网站时，需要首先通过域名解析服务（DNS）获得网站的IP。域名解析通常不是一次性完成的，常常需要查询若干不同的域名服务器才能找到对应的IP。如下图所示，用户首先在本地配置一个本地DNS服务器地址，本地DNS服务器收到DNS请求后若不能解析，会将请求转发给更高一级的DNS服务器直到找到域名对应的IP或确定域名不存在



普通的访问流程：



加入GBLS（全局负载均衡设备）访问流程:



优点:部署容易，成本低。缺点:GSLB只能拿到本地DNS的IP（获取的地理信息或者其他信息）,不能拿到用户的IP，或者说用户的指定的本地DNS如果离自己较远,那么,GSLB就回错误的认为你在本地DNS处,然后返回错误的IP。

2、HTTP重定向负载均衡:

使用基于HTTP重定向方案，首先在DNS中将GSLB设备的IP地址登记为域名的A记录（既域名对应的IP）。如上图所示，用户首先通过DNS得到GSLB设备的IP地址，此时用户以为这就是站点服务器的IP，并向其发送HTTP请求。GSLB设备收到HTTP请求后使用一定策略选择一个最合适的服务器，然后GSLB设备向用户发送一个HTTP重定向指令（HTTP302），并附上选出的服务器的IP地址。最后，用户根据重定向IP访问站点的服务器。



3、基于IP欺骗的负载均衡:



5、和单机结构,集群结构的区别:
从单机到集群,你项目的架构,代码基本不用动(也会改动,因为服务器之间的响应也需要你的项目中有对应的模块或者功能),因为你是通过服务器的数量和负载均衡来增加并发和大流量问题的.

举个栗子:电商网站的某一款娃娃热卖((@^_^@)),负责娃娃的模块瞬间生热,多台服务器也满足不了大家只买这一款娃娃,然而网站的其他模块或者部分基本闲着,调度者(负载均衡器)也表示爱莫能助(因为每一台服务器的局部发热严重),已经非常平衡的分配了用户请求,但是请求的是都是娃娃,这一模块卡顿甚至500(http状态码都5开头了),也不能满足用户需求. 明显是需要把所有的节点服务器中空余的资源分出一部分来给娃娃模块.

6、分布式结构工作机理:
分布式结构就是将一个完整的系统,按照业务功能,拆分成一个个独立的子系统,在分布式结构中,每个子系统就被称为“服务”.这些子系统能够独立运行在web容器中,它们之间通过RPC方式通.

如果上述的网站采用分布式结构,你的服务器器们不在做同一件事情,各司其职,娃娃卖的好,那就多用几台服务器来负责娃娃相关的模块,利用率得到提高.



7、分布式结构的优点:
<1>系统之间的耦合度大大降低,可以独立开发、独立部署、独立测试,系统与系统之间的边界非常明确,排错也变得相当容易,开发效率大大提升

<2>系统之间的耦合度降低,从而系统更易于扩展.我们可以针对性地扩展某些服务.假设这个商城要搞一次大促,下单量可能会大大提升,因此我们可以针对性地提升订单系统、产品系统的节点数量,而对于后台管理系统、数据分析系统而言,节点数量维持原有水平即可

<3>服务的复用性更高.比如,当我们将用户系统作为单独的服务后,该公司所有的产品都可以使用该系统作为用户系统,无需重复开发

CDN
1、什么是CDN？
内容分发网络（Content Delivery Network, CDN）其目的是通过在现有的Internet中增加一层新的网络架构，将网站的内容发布到最接近用户的网络”边缘”，使用户可以就近取得所需的内容，解决Internet网络拥塞状况，提高用户访问网站的响应速度。从技术上全面解决由于网络带宽小、用户访问量大、网点分布不均等原因所造成的用户访问网站响应速度慢的问题。

2、CDN组成有什么?
一个CDN网络主要由以下几部分组成：内容缓存设备、内容分发管理设备、本地负载均衡交换机、GSLB设备和CDN管理系统，其网络结构如下图所示



各部分的工作:

1、内容缓存设备Cache用于缓存内容实体和对缓存内容进行组织和管理。当有用户访问该客户内容时，直接由各缓存服务器响应用户的请求

2、内容分发管理设备主要负责核心Web服务器内容到CDN网络内缓存设备的内容推送、删除、校验以及内容的管理、同步。

3、GSLB设备则实现CDN全网各缓存节点之间的资源负载均衡，它与各节点的SLB设备保持通信，搜集各节点缓存设备的健康状态、性能、负载等，自动将用户指引到位于其地理区域中的服务器或者引导用户离开拥挤的网络和服务器。还可以通过使用多站点的内容和服务来提高容错性和可用性，防止因本地网或区域网络中断、断电或自然灾害而导致的故障。

3、CDN的工作流程
用户访问某个站点的内容时，若该站点使用了CDN网络，则在用户会在域名解析时获得CDN网络GSLB设备的IP地址。GSLB设备根据其预设的选择策略（如，地理区域、用户时间等）为用户选择最合适的内容缓存节点，并且使用某种方式（如，基于DNS、基于HTTP重定向、基于IP欺骗的方式等）导引用户访问所选的内容缓存节点。用户继续向缓存节点发出请求，若缓存中包含请求的内容，则直接返回给用户，否则从核心Web服务器中获取该内容，缓存后返回给用户。这样当用户再次访问相同内容或其他用户访问相同内容时，可以直接从缓存中读取，提高了效率

写在最后:
集群强调的是任务的同一性,分布式强调的是差异性.但是不是绝对对立的.上海的服务器处理上海的文件上传事务,北京服务器处理北京文件上传事务,最终结果都完成文件上传,从全局考虑是分布式的结构,但是就北京而言,北京那边有多台服务器在做同样的事情那么北京那边就是集群结构…分布式的每一个节点都可以做集群,但是集群不一定就是分布式的.

# 大数据之大型网站高并发处理

# **主要是由Nginx实现**

## 一，产生背景

拿我们日常生活会遇到的一些问题：

**大学读书时，每到选修课的时候，学校的选课系统，卡顿，或者直接挂掉**

![img](https://pic3.zhimg.com/80/v2-6b5af0b0f105278fddeb4a48f51a9056_720w.jpg)

**淘宝，京东等商场活动，双11，京东618**

![img](https://pic4.zhimg.com/80/v2-3a550c6ade48aabf82c58cab0055b6d7_720w.jpg)

12306网站 购票压力



分析原因：



上述场景产生的主要2大原因:



1.巨大流量—海量的并发访问



2.单台服务器资源和能力有限



## **二，负载均衡（Load Balance）**

在解释负载均衡前，我们首先得弄清楚什么是高并发。



2.1 高并发

见名知意，高（大量的），并发就是可以使用多个线程或者多个进程，同时处理（就是并发）不同的操作。简而言之就是每秒内有多少个请求同时访问。



2.2 负载均衡

负载均衡：将请求/数据【均匀】分摊到多个操作单元上执行，负载均衡的关键在于【均匀】,也是分布式系统架构设计中必须考虑的因素之一。



2.3 tomcat并发图

![img](https://pic4.zhimg.com/80/v2-8b5122f232750a8b2b622ffd04dbe05b_720w.jpg)

由tomcat的并发测试图可以发现，当每秒300个请求同时访问tomcat时，tomcat已经开始承受不住，出现波动。那么大型网站是如何处理高并发的呢？以下是高并发场景下，实现负载均衡的一个分布式架构图。

见互联网分布式架构，分为客户端层、反向代理nginx层、站点层、服务层、数据层。只需要实现“将请求/数据 均匀分摊到多个操作单元上执行”，就能实现负载均衡。



## 三，Nginx简介

3.1 什么是 Nginx?

Nginx是一款 轻量级 的Web服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器。



由俄罗斯的程序设计师Igor Sysoev所开发，其特点是占有内存少，并发能力强，nginx的并发能力确实在同类型的网页服务器中表现非常好。



•2004年10月4日 第一个公开版本0.1.0发布。其将源代码以类BSD许可证的形式发布。



•官方测试nginx能够支撑5万并发链接，并且CPU、内存等资源消耗却非常低，运行非常稳定。







## 四，Nginx对比Apache

Nginx和apache的优缺点:

1.nginx相对于apache的优点：

轻量级，同样起web 服务，比apache 占用更少的内存及资源高并发，nginx 处理请求是异步非阻塞（如前端ajax）的，而 apache 则是阻塞型的，在高并发下nginx能保持低资源低消耗高性能高度模块化的设计，编写模块相对简单

还有，它社区活跃，各种高性能模块出品迅速（十几年时间发展）

2.apache 相对于nginx 的优点：

Rewrite重写 ，比nginx 的rewrite 强大模块超多，基本想到的都可以找到。少bug ，nginx 的bug 相对较多。（出身好起步高）

3.Nginx 配置简洁, Apache 复杂



## 五，安装Nginx

5.1 安装依赖

Nginx依赖 gcc openssl-devel pcre-devel zlib-devel

安装命令：yum -y install gcc openssl-devel pcre-devel zlib-devel

```text
yum -y install gcc openssl-devel pcre-devel zlib-devel
```

### 5.2 解压文件

上传压缩包 —>下载连接：[http://nginx.org](https://link.zhihu.com/?target=http%3A//nginx.org/) (我用的是1.8.1)

解压命令：

```text
tar -zxvf nginx-1.8.1.tar.gz
```

### 5.3 configure配置

进入解压后的源码目录，然后执行configure命令进行配置:

命令：

```text
./configure
```

### 5.4 编译并安装

命令

```text
make && make install
```

安装好后，会在/usr/soft下生成nginx目录(这是我编译前指定的)，

### 5.5 配置Nginx为系统服务，以方便管理

**1、在/etc/rc.d/init.d/目录中建立文本文件nginx**

命令：

```text
cd /etc/rc.d/init.d
vi nginx
```

**2、在文件中粘贴下面的内容：**

```text
#!/bin/sh
#
# nginx - this script starts and stops the nginx daemon
#
# chkconfig:   - 85 15 
# description:  Nginx is an HTTP(S) server, HTTP(S) reverse \
#               proxy and IMAP/POP3 proxy server
# processname: nginx
# config:      /etc/nginx/nginx.conf
# config:      /etc/sysconfig/nginx
# pidfile:     /var/run/nginx.pid
 
# Source function library.
. /etc/rc.d/init.d/functions
 
# Source networking configuration.
. /etc/sysconfig/network
 
# Check that networking is up.
[ "$NETWORKING" = "no" ] && exit 0
 
nginx="/usr/local/nginx/sbin/nginx"
prog=$(basename $nginx)
 
NGINX_CONF_FILE="/usr/local/nginx/conf/nginx.conf"
 
[ -f /etc/sysconfig/nginx ] && . /etc/sysconfig/nginx
 
lockfile=/var/lock/subsys/nginx
 
make_dirs() {
   # make required directories
   user=`nginx -V 2>&1 | grep "configure arguments:" | sed 's/[^*]*--user=\([^ ]*\).*/\1/g' -`
   options=`$nginx -V 2>&1 | grep 'configure arguments:'`
   for opt in $options; do
       if [ `echo $opt | grep '.*-temp-path'` ]; then
           value=`echo $opt | cut -d "=" -f 2`
           if [ ! -d "$value" ]; then
               # echo "creating" $value
               mkdir -p $value && chown -R $user $value
           fi
       fi
   done
}
 
start() {
    [ -x $nginx ] || exit 5
    [ -f $NGINX_CONF_FILE ] || exit 6
    make_dirs
    echo -n $"Starting $prog: "
    daemon $nginx -c $NGINX_CONF_FILE
    retval=$?
    echo
    [ $retval -eq 0 ] && touch $lockfile
    return $retval
}
 
stop() {
    echo -n $"Stopping $prog: "
    killproc $prog -QUIT
    retval=$?
    echo
    [ $retval -eq 0 ] && rm -f $lockfile
    return $retval
}
 
restart() {
    configtest || return $?
    stop
    sleep 1
    start
}
 
reload() {
    configtest || return $?
    echo -n $"Reloading $prog: "
    killproc $nginx -HUP
    RETVAL=$?
    echo
}
 
force_reload() {
    restart
}
 
configtest() {
  $nginx -t -c $NGINX_CONF_FILE
}
 
rh_status() {
    status $prog
}
 
rh_status_q() {
    rh_status >/dev/null 2>&1
}
 
case "$1" in
    start)
        rh_status_q && exit 0
        $1
        ;;
    stop)
        rh_status_q || exit 0
        $1
        ;;
    restart|configtest)
        $1
        ;;
    reload)
        rh_status_q || exit 7
        $1
        ;;
    force-reload)
        force_reload
        ;;
    status)
        rh_status
        ;;
    condrestart|try-restart)
        rh_status_q || exit 0
            ;;
    *)
        echo $"Usage: $0 {start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest}"
        exit 2
esac
```

**3、修改nginx文件的执行权限**

命令：

```text
chmod +x nginx
```

**4、添加该文件到系统服务中去**

添加命令：

```text
chkconfig  --add  nginx
```

查看是否添加成功:

```text
chkconfig  --list  nginx
```

**启动：重启：停止命令:**

```text
service nginx  start|restart|stop
```

## 六，Nginx配置

### 6.1 nginx默认配置详解

```text
#进程数，建议设置和CPU个数一样或2倍
worker_processes  2;

#日志级别
error_log  logs/error.log  warning;(默认error级别)

# nginx 启动后的pid 存放位置
#pid        logs/nginx.pid;

events {
	#配置每个进程的连接数，总的连接数= worker_processes * worker_connections
    #默认1024
    worker_connections  10240;
}

http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;

#连接超时时间，单位秒
keepalive_timeout  65;

    server {
        listen       80;
        server_name  localhost                 
        #默认请求
  		location / {
    				 root  html;   #定义服务器的默认网站根目录位置
   				  index  index.php index.html index.htm;  #定义首页索引文件的名称
        }
	    #定义错误提示页面
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
}
```

6.2 负载均衡配置

nginx支持以下负载均衡机制（或方法）：



a) 轮询负载均衡 - 对应用程序服务器的请求以循环方式分发，



b) 加权负载均衡



c) 最少连接数 - 将下一个请求分配给活动连接数最少的服务器



d) ip-hash - 哈希函数用于确定下一个请求（基于客户端的IP地址）应该选择哪个服务器。



6.2.1 默认负载平衡配置

使用nginx进行负载平衡的最简单配置如下所示：

```text
http { 
    upstream demo{ 
        server node01; //内部服务器
        server node02; 
        server node03; 
    } 

    server { 
        listen 80; 
	server_name  localhost;
        location / {
            proxy_pass http://demo;
        }
    } 
}
```

在上面的示例中，在srv1-srv3上运行相同应用程序的3个实例。如果没有专门配置负载均衡方法，则默认为循环法。

所有请求都被 代理到服务器组demo，并且nginx应用HTTP负载平衡来分发请求。

node01 一次 ， node02 一次， node03 一次 …

## 6.2.2 加权负载平衡

通过使用服务器权重，还可以进一步影响nginx负载均衡算法，谁的权重越大，分发到的请求就越多

```text
upstream demo {
        server srv1.example.com weight=3;  
        server srv2.example.com;  //默认是 1  1-10范围
        server srv3.example.com;
  }
```

### 6.2.3 最少连接负载平衡

在连接负载最少的情况下，nginx会尽量避免将过多的请求分发给繁忙的应用程序服务器，而是将新请求分发给不太繁忙的服务器，避免服务器过载。

相对来说这种方式有点鸡肋…

```text
upstream demo {
        least_conn;
        server srv1.example.com;
        server srv2.example.com;
        server srv3.example.com;
    }
```

## 6.2.4 会话持久性

上述的循环或最少连接数的负载平衡方法，每个后续客户端的请求都可能被分发到不同的服务器。不能保证相同的客户端总是定向到相同的服务器。如果需要将客户端绑定到特定的应用程序服务器 - 换句话说，就是始终选择相同的服务器而言，就要使客户端的会话“粘滞”或“持久” 。

ip-hash负载平衡机制就是有这种特性。使用ip-hash，客户端的IP地址将用作散列键，以确定应该为客户端的请求选择服务器组中的哪台服务器。

此方法可确保来自同一客户端的请求将始终定向到同一台服务器，除非此服务器不可用。

```text
upstream demo{
    ip_hash;
    server srv1.example.com;
    server srv2.example.com;
    server srv3.example.com;
}
```

### 6.3 **Nginx的访问控制**

Nginx还可以对IP的访问进行控制，**allow**代表允许，**deny**代表禁止.

```text
location / {
    deny 192.168.2.180;
    allow 192.168.78.0/24;
    allow 10.1.1.0/16;
    allow 192.168.1.0/32;
    deny all;
    proxy_pass http://shsxt;
}
```

从上到下的顺序，匹配到了便跳出。如上的例子先禁止了1个，接下来允许了3个网段，其中包含了一个ipv6，最后未匹配的IP全部禁止访问.



## 七，虚拟主机

7.1 什么是虚拟主机？

虚拟主机是指在网络服务器上分出一定的磁盘空间，用户可以租用此部分空间，供用户放置站点及应用组件，提供必要的数据存放和传输功能。

说白了虚拟主机就是把一台物理服务器划分成多个“虚拟”的服务器，各个服务器之间完全独立，在外界看来，每一台虚拟主机和一台单独的主机的表现完全相同。

所以这种被虚拟化的逻辑主机被形象的成为 “虚拟主机”.

优点？

由于多台虚拟主机共享一台真实主机的资源，每个虚拟主机用户承受的硬件费用、网络维护费用、通信线路的费用均大幅度降低。许多企业建立网站都采用这种方法，这样不仅大大节省了购买机器和租用专线的费用，网站服务器管理简单，诸如软件配置、防病毒、防攻击等安全措施都由专业服务商提供，大大简化了服务器管理的复杂性；同时也不必为使用和维护服务器的技术问题担心，更不必聘用专门的管理人员。

类别：

基于域名的虚拟主机，通过域名来区分虚拟主机

基于端口的虚拟主机，通过端口来区分虚拟主机

基于ip 的虚拟主机，很少用。

7.2 基于域名的虚拟主机

```text
http { 
    upstream demo{ 
        server node01; 
	} 
	upstream test{ 
        server node03; 
     } 
 	server { 
        listen 80; 
//访问demo.com的时候，会把请求导到demo的服务器组里
		server_name  demo.com;
        location / {
            proxy_pass http://demo;
        }
	} 
    server { 
        listen 80; 
	//访问test.com的时候，会把请求导到test的服务器组里
	server_name  test.com; 
        location / {
            proxy_pass http://test;
        }
	} 
}
```

node01, node03 是在虚拟机中配置的ip别名

可在 /etc/hosts中配置

```text
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.150.101  node01
192.168.150.102  node02
192.168.150.103  node03
```

**注意：基于域名的虚拟机主机 在模拟应用场景时，需要在windows系统的hosts文件里配置域名映射。（C:\Windows\System32\drivers\etc\hosts）**

```text
192.168.150.101  test.com
192.168.150.103  demo.com
```

启动nginx，分别访问，demo.test 和[test.com](https://link.zhihu.com/?target=http%3A//test.com/)

7.3 基于端口的虚拟主机

```text
upstream demo{
        server node03;
    }

    upstream test{
        server node01;
    }


    server {
		//当访问nginx的 81端口时，将请求分发到 test组
        listen 81;
        server_name  localhost;
        location / {
            proxy_pass http://test;
        }
    }


    server {
		//当访问nginx的 80端口时，将请求分发到  demo组
        listen       80;
        server_name  localhost;

        location / {
            proxy_pass http://demo;
        }

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
```

## 八，正向代理和反向代理

正向代理

举个栗子：我是一个用户，我访问不了某网站，但是我能访问一个代理服务器，这个代理服务器呢,他能访问那个我不能访问的网站，于是我先连上代理服务器,告诉他我需要那个无法访问网站的内容，代理服务器去取回来,然后返回给我。像我们经常通过vpn访问国外的网站，此时就是正向代理。

客户端必须设置正向代理服务器，当然前提是要知道正向代理服务器的IP地址，还有代理程序的端口。

反向代理

反向代理方式是指以代理服务器来接收internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。

反向代理隐藏了真实的服务端，当我们请求 [百度一下，你就知道](https://link.zhihu.com/?target=http%3A//www.baidu.com/) 的时候，就像拨打 10086 一样，背后可能有成千上万台服务器为我们服务，但具体是哪一台，你不知道，也不需要知道，你只需要知道反向代理服务器是谁就好了，[百度一下，你就知道](https://link.zhihu.com/?target=http%3A//www.baidu.com/) 就是我们的反向代理服务器，反向代理服务器会帮我们把请求转发到真实的服务器那里去。Nginx 就是性能非常好的反向代理服务器，用来做负载均衡。

## 九，Nginx session一致性问题

http协议是无状态的，即你连续访问某个网页100次和访问1次对服务器来说是没有区别对待的，因为它记不住你。

那么，在一些场合，确实需要服务器记住当前用户怎么办？

比如用户登录邮箱后，接下来要收邮件、写邮件，总不能每次操作都让用户输入用户名和密码吧，为了解决这个问题，session的方案就被提了出来，事实上它并不是什么新技术，而且也不能脱离http协议以及任何现有的web技术.

session的常见实现形式是会话cookie（session cookie），即未设置过期时间的cookie，这个cookie的默认生命周期为浏览器会话期间，只要关闭浏览器窗口，cookie就消失了。

9.1 Session共享

首先我们应该明白，为什么要实现共享，如果你的网站是存放在一个机器上，那么是不存在这个问题的，因为会话数据就在这台机器，但是如果你使用了负载均衡把请求分发到不同的机器呢？这个时候会话id在客户端是没有问题的，但是如果用户的两次请求到了两台不同的机器，而它的session数据可能存在其中一台机器，这个时候就会出现取不到session数据的情况，于是session的共享就成了一个问题

9.2 Session一致性解决方案

– 1、session复制

tomcat 本身带有复制session的功能(这里不做介绍)。

– 2、 共享session

需要专门管理session的软件，

memcached 缓存服务，可以和tomcat整合，帮助tomcat共享管理session。

9.3 安装 memcached

安装memcached内存数据库

命令：

```text
yum -y install memcached
```

启动的命令:

```text
memcached -d -m 128m -p 11211 -l 你安装memcached的ip -u 用户名 -P 密码/没有为空 /tmp/
## tmp 是存放路径
-p  端口号
-d  以daemon方式运行 --守护线程(后台运行)
-m  允许最大内存用量，单位是M （默认是64M）
```

停止命令：

```text
[root@node03 lib]# ps -ef|grep memcached
root       1536      1  0 02:34 ?        00:00:00 memcached -d -m 128m -p 11211 -l 192.168.150.103 -u root -P /tmp/
root       1684   1168  0 03:08 pts/0    00:00:00 grep memcached
[root@node03 lib]# kill -9 1536
```

### web服务器连接memcached的jar包拷贝到tomcat的lib

我这里用的tomcat 只要把相关jar传到tomcat的 lib目录下即可

### 配置tomcat的conf目录下的context.xml

注意：每个被nginx代理的tomcat都需要配置

```text
<Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager"
    memcachedNodes="n1:   ====##你配置memcached的虚拟机ip##===   :11211"
    sticky="true"
    lockingMode="auto"
    sessionBackupAsync="false"
   requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"
sessionBackupTimeout="1000" transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" />
```

配置memcachedNodes属性，配置memcached数据库的ip和端口，默认11211，多个的话用逗号隔开.

目的是为了让tomcat服务器从memcached缓存里面拿session或者是放session.

修改tomcat中的index.jsp，取sessionid看一看

```text
<%@ page language="java" contentType="text/html; charset=UTF-8"  pageEncoding="UTF-8"%>
<html lang="en">
SessionID:<%=session.getId()%>
</br>
SessionIP:<%=request.getServerName()%>
</br>
<h1>tomcat1</h1>
</html>
```

**可以看到虽然每次请求 nginx 分发到的服务器不是一个，但是sessionID始终唯一，证明session共享成功实现**

